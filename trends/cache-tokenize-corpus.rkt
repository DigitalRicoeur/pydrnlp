#lang racket

(require racket/runtime-path
         racket/fixnum
         db
         sql
         racket/fasl
         ricoeur/tei
         "tokenize-one-doc.rkt"
         "worker.rkt"
         "types.rkt"
         pydrnlp/support
         racket/stxparam
         syntax/parse/define
         (for-syntax racket/base
                     syntax/transformer))

(provide (contract-out
          [get/build-tokenized-corpus
           (->* [(instance-set/c tei-document?)]
                [#:quiet? any/c]
                tokenized-corpus?)]
          ))

(define-runtime-path trends-cache-db.sqlite
  "trends-cache-db.sqlite")

(define (get/build-tokenized-corpus docs #:quiet? [quiet? #t])
  ;; Main entry point.
  ;; Connects to the the database when using the cache,
  ;; and always creates a custodian to implicitly
  ;; manage the (potential) Python worker.
  (define cust (make-custodian))
  (define db
    (when trends-engine-revision
      (sqlite3-connect #:database trends-cache-db.sqlite
                       #:use-place #t
                       #:mode 'create)))
  (dynamic-wind
   void
   (λ ()
     (parameterize ([current-custodian cust])
       (cond
         [trends-engine-revision
           (call-with-transaction
            db
            (λ ()
              (query-exec db "PRAGMA foreign_keys = ON")
              (check-cache-db-format! db)
              (do-get/build docs #:db db #:quiet? quiet?)))]
         [else
          ;; not using cache, but we still want a custodian for Python
          (do-get/build docs #:db #false #:quiet? quiet?)])))
   (λ ()
     (custodian-shutdown-all cust)
     (when trends-engine-revision
       (disconnect db)))))


(define-simple-macro (define/db (name:id kw-formal ...) body:expr ...+)
  (define (name #:db db kw-formal ...)
    (syntax-parameterize ([local-db (make-variable-like-transformer #'db)])
      body ...)))

(define/db (do-get/build docs #:quiet? quiet?)
  ;; We first check to see whether a whole corpus
  ;; is in the cache and ready to use.
  ;; In that happy case, we don't even need to start Python.
  (define current-TokenizerRevisionFasl
    (s-exp->fasl trends-engine-revision))
  (define checksum-table
    (tei-document-set->checksum-table docs))
  (define current-corpusChecksumTableFasl
    (s-exp->fasl checksum-table))
  (match (and trends-engine-revision ;; are we even using the cache?
              (query-maybe-row/db
               ;; Look for a row for the this corpus
               ;; generated by this revision of the tokenizer.
               (select corpusLemmaCountBlob
                       corpusLemmaStringBlob
                       #:from tTokenizedCorpora
                       #:where
                       (= corpusChecksumTableFasl
                          ,current-corpusChecksumTableFasl)
                       (= cacheTokenizerRevisionFasl
                          ,current-TokenizerRevisionFasl))))
    [(vector counts-blob strings-blob)
     ;; YES! We can just load the corpus from the cache.
     (define plain-info
       (instance-set->plain docs))
     (define table
       (blobs->lemma-table blobs->lemma-table))
     ;; Mark the access time for the corpus and documents
     ;; so we don't evict them from the cache:
     (define atime (current-seconds)) 
     (query-exec/db
      (update tTokenizedCorpora
              #:set [accessedPosix ,atime]
              #:where
              (= corpusChecksumTableFasl
                 ,current-corpusChecksumTableFasl)
              (= cacheTokenizerRevisionFasl
                 ,current-TokenizerRevisionFasl)))
     (query-exec/db
      (update tTokenizedDocuments
              #:set [accessedPosix ,atime]
              #:where
              (exists
               (select 1
                       #:from tTokenizedCorporaDocuments
                       #:where
                       (= tTokenizedDocuments.docTitle
                          tTokenizedCorporaDocuments.docTitle)
                       (= tTokenizedDocuments.docChecksum
                          tTokenizedCorporaDocuments.docChecksum)
                       (= corpusChecksumTableFasl
                          ,current-corpusChecksumTableFasl)
                       (= cacheTokenizerRevisionFasl
                          ,current-TokenizerRevisionFasl)))))
     ;; Get the info for the individual documents.
     ;; We can avoid loading the string blob,
     ;; since we have the full one cached.
     (define tokenized-docs
       (for/instance-set
           ([{title-str blob}
             (in-query/db
              (select docTitle docLemmaCountBlob
                      #:from (inner-join tTokenizedDocuments
                                         tTokenizedCorporaDocuments
                                         #:natural)
                      #:where
                      (= corpusChecksumTableFasl
                         ,current-corpusChecksumTableFasl)
                      (= cacheTokenizerRevisionFasl
                         ,current-TokenizerRevisionFasl)))])
         (tokenized-document
          (instance-set-ref plain-info (string->symbol title-str))
          (blob->lemma/count blob))))
     ;; Done; return the corpus.
     (tokenized-corpus table
                       tokenized-docs)]
    [_
     ;; Cache miss for the full corpus,
     ;; so we'll need to assemble it from parts.
     (build-tokenized-corpus docs
                             #:db local-db
                             #:revision-fasl current-TokenizerRevisionFasl
                             #:checksum-table-fasl current-corpusChecksumTableFasl
                             #:checksum-table checksum-table
                             #:quiet? quiet?)]))



(define/db (get-cached-documents docs
                                 #:atime atime
                                 #:revision-fasl current-TokenizerRevisionFasl)
  ;; (Called from `build-tokenized-corpus`.)
  ;; Here, we are looking for any individual documents
  ;; that might be able to reuse from the cache,
  ;; even though the whole corpus wasn't there.
  (define wanted-ast
    ;; the docs we're interested in, by title and checksum
    (make-values*-table-expr-ast
     (for/list ([d (in-instance-set docs)])
       (list (value->scalar-expr-ast (instance-title d))
             (value->scalar-expr-ast (symbol->string
                                      (tei-document-checksum d)))))))
  ;; First we mark the access time, so we can return directly:
  (query-exec/db
   (update tTokenizedDocuments
           #:set [accessedPosix ,atime]
           #:where
           (= cacheTokenizerRevisionFasl ,current-TokenizerRevisionFasl)
           (in (%row docTitle docChecksum)
               #:from (TableExpr:AST ,wanted-ast))))
  ;; Now we gather the documents:
  (for/fold ([found (instance-set)]
             [table empty-lemma-table])
            ([{title-str counts-blob strings-blob}
              (in-query/db
               (select docTitle docLemmaCountBlob docLemmaStringBlob
                       #:from tTokenizedDocuments
                       #:where
                       (= cacheTokenizerRevisionFasl ,current-TokenizerRevisionFasl)
                       (in (%row docTitle docChecksum)
                           #:from (TableExpr:AST ,wanted-ast))))])
    (define doc-tbl
      (blobs->lemma-table counts-blob strings-blob))
    (define doc
      (tokenized-document (get-plain-instance-info
                           (instance-set-ref docs (string->symbol title-str)))
                          (lemma-table->lemma/count doc-tbl)))
    (values (set-add found doc)
            (lemma-table-union table doc-tbl))))




(define/db (build-tokenized-corpus docs
                                   #:revision-fasl current-TokenizerRevisionFasl
                                   #:checksum-table-fasl current-corpusChecksumTableFasl
                                   #:checksum-table checksum-table
                                   #:quiet? quiet?)
  ;; We didn't have the full corpus in the cache,
  ;; so we must build it up from documents.
  ;; We first collect any individual documents that might be in the cache ...
  (define atime (current-seconds))
  (define-values [found table]
    (if trends-engine-revision ;; ... assuming we're even using the cache.
        (get-cached-documents docs
                              #:db local-db
                              #:atime atime
                              #:revision-fasl current-TokenizerRevisionFasl)
        (values (instance-set) empty-lemma-table)))
  (define docs-to-do
    (set-subtract docs found))
  ;; Start a Python worker:
  (define py (launch-trends-engine #:quiet? quiet?))
  ;; Now we handle the documents that weren't in the cache.
  (define corpus
    (for/fold ([found found]
               [table table]
               #:result (tokenized-corpus table found))
              ([d (in-instance-set docs-to-do)])
      (define info (get-plain-instance-info d))
      (define checksum-str (symbol->string (tei-document-checksum d)))
      (match-define doc-table
        (tokenize-one-doc d #:tokenizer py))
      (define tokenized-doc
        (tokenized-document info (lemma-table->lemma/count doc-table)))
      ;; If we're actually using the cache at all,
      ;; save this document.
      (when trends-engine-revision
        (define-values [counts-blob strings-blob]
          (lemma-table->blobs doc-table))
        (query-exec/db
         (insert #:into tTokenizedDocuments
                 #:set 
                 [docTitle ,(instance-title info)]
                 [docChecksum ,checksum-str]
                 [cacheTokenizerRevisionFasl ,current-TokenizerRevisionFasl]
                 [docLemmaCountBlob ,counts-blob]
                 [docLemmaStringBlob ,strings-blob]
                 [createdPosix ,atime]
                 [accessedPosix ,atime])))
      (values (set-add found tokenized-doc)
              (lemma-table-union table doc-table))))
  ;; Ok! We've tokenized all the documents,
  ;; so we're done with the Python worker:
  (python-worker-kill py)
  (begin0 corpus
    ;; Last step before we return is to save the whole corpus,
    ;; if we're using the cache.
    (when trends-engine-revision
      (define-values [counts-blob strings-blob]
        (lemma-table->blobs (tokenized-corpus-lemma-table corpus)))
      ;; save the corpus
      (query-exec/db
       (insert #:into tTokenizedCorpora
               #:set
               [corpusChecksumTableFasl ,current-corpusChecksumTableFasl]
               [cacheTokenizerRevisionFasl ,current-TokenizerRevisionFasl]
               [corpusLemmaCountBlob ,counts-blob]
               [corpusLemmaStringBlob ,strings-blob]
               [createdPosix ,atime]
               [accessedPosix ,atime]))
      ;; save rows linking the corpus to its documents
      (query-exec/db
       (for/insert-statement (#:into tTokenizedCorporaDocuments
                              [{title-sym checksum-sym}
                               (in-immutable-hash checksum-table)])
         #:set
         [corpusChecksumTableFasl ,current-corpusChecksumTableFasl]
         [cacheTokenizerRevisionFasl ,current-TokenizerRevisionFasl]
         [docTitle ,(symbol->string title-sym)]
         [docChecksum ,(symbol->string checksum-sym)])))))




                                        
;                                         ;; 
;                                         ;; 
;    ;; ;;; ;;  ; ;;   ; ;;    ;;;   ;; ;;;;;
;  ;;  ; ;; ;;  ;;  ;  ;;  ;  ;   ;  ;;;  ;; 
;   ;    ;; ;;  ;;  ;  ;;  ;  ;   ;  ;;   ;; 
;    ;;  ;; ;;  ;;  ;; ;;  ;;;;   ;; ;;   ;; 
;      ;;;; ;;  ;;  ;  ;;  ;  ;   ;  ;;   ;; 
;  ;   ;  ; ;;  ;;  ;  ;;  ;  ;   ;  ;;    ; 
;   ;;;   ;;;;  ;;;;   ;;;;    ;;;   ;;    ;;
;               ;;     ;;                    
;               ;;     ;;                    
;               ;;     ;;                    
;                                            


(define (tei-document-set->checksum-table docs)
  (TODO/void tei-document-set->checksum-table #: probably better elsewhere)
  (for/hasheq ([doc (in-instance-set docs)])
    (values (instance-title/symbol doc)
            (tei-document-checksum doc))))

(define-syntax-parameter local-db
  (λ (stx) (raise-syntax-error #f "used out of context" stx)))

(define-simple-macro (define-query/db (~seq name/db:id name:id) ...)
  (begin (define-simple-macro (name/db s:expr)
           (name local-db s))
         ...))

(define-query/db
  query-exec/db query-exec
  query-maybe-row/db query-maybe-row
  query-row/db query-row
  query-list/db query-list
  query-maybe-value/db query-maybe-value
  query-value/db query-value)

(define-sequence-syntax in-query/db
  (λ (stx) (raise-syntax-error #f "only allowed in for-like clauses" stx))
  (syntax-parser
    [[lhs (_ arg ...+)]
     #'[lhs (in-query local-db arg ...)]]))



(define (check-cache-db-format! db)
  (define ident-ast:tCacheDbFormatVersion
    (ident-qq tCacheFormatVersion))
  (match (and (table-exists? db (ast->sqlite3 ident-ast:tCacheDbFormatVersion))
              (query-list
               db
               (select cacheDbFormatVersion
                       #:from (Ident:AST ,ident-ast:tCacheDbFormatVersion))))
    [(list it)
     #:when (eqv? it cache-db-format-version)
     (void)]
    [_
     (query-exec db (ident-ast->drop-statement ident-ast:tCacheDbFormatVersion))
     (for ([stmnt (in-list drop-cache-tables-statements)])
       (query-exec db stmnt))
     (query-exec db
                 (create-table (Ident:AST ,ident-ast:tCacheDbFormatVersion)
                               #:columns [cacheDbFormatVersion int #:not-null]))
     (query-exec db
                 (insert #:into (Ident:AST ,ident-ast:tCacheDbFormatVersion)
                         #:set [cacheDbFormatVersion ,cache-db-format-version]))
     (for ([stmnt (in-list create-cache-tables-statements)])
       (query-exec db stmnt))]))


(define (ast->sqlite3 ast)
  (sql-ast->string ast 'sqlite3))

(define (ident-ast->drop-statement ast)
  (string-append "DROP TABLE IF EXISTS "
                 (ast->sqlite3 ast)))

(TODO/void for/insert-statement #: avoid copying code)

(define-syntax-parser for/insert-statement
  ;; copied and pasted from ricoeur/tei/search/postgresql
  [(_ (#:into name-ast-form
       for-clause ...)
      body-or-break ...
      #:set
      [column-ident-form
       scalar-expr-form] ...)
   #`(insert
      #:into name-ast-form
      #:columns column-ident-form ...
      #:from
      (TableExpr:AST
       ,(make-values*-table-expr-ast
         (for/fold/derived #,this-syntax
                           ([so-far '()])
           (for-clause ...)
           body-or-break ...
           (cons (list (scalar-expr-qq scalar-expr-form)
                       ...)
                 so-far)))))])



(define-syntax-parser define-cache-tables
  #:literals {create-table}
  #:track-literals
  [(_ {(~alt (~optional (~seq #:create create-cache-tables-statements:id))
             (~optional (~seq #:drop drop-cache-tables-statements:id))
             (~optional (~seq #:clear clear-cache-tables-statements:id)))
       ...}
      (create-table create-name:id create-body ...)
      ...)
   #:with (delete-name ...) (reverse (syntax->list #'(create-name ...)))
   #'(begin
       (~? (define create-cache-tables-statements
             (list (create-table create-name create-body ...) ...)))
       (~? (define clear-cache-tables-statements
             (list (delete #:from delete-name) ...)))
       (~? (define drop-cache-tables-statements
             (map ident-ast->drop-statement
                  (list (ident-qq delete-name) ...)))))])

(define/contract cache-db-format-version
  fixnum-for-every-system?
  2)

(define-cache-tables {#:create create-cache-tables-statements
                      #:drop drop-cache-tables-statements}
  (create-table tTokenizedDocuments
                #:columns
                [docTitle text #:not-null]
                [docChecksum text #:not-null]
                [cacheTokenizerRevisionFasl blob #:not-null]
                [docLemmaCountBlob blob #:not-null]
                [docLemmaStringBlob blob #:not-null]
                [createdPosix int #:not-null]
                [accessedPosix int #:not-null]
                #:constraints
                (primary-key docTitle docChecksum cacheTokenizerRevisionFasl))
  (create-table tTokenizedCorpora
                #:columns
                [corpusChecksumTableFasl blob #:not-null]
                [cacheTokenizerRevisionFasl blob #:not-null]
                [corpusLemmaCountBlob blob #:not-null]
                [corpusLemmaStringBlob blob #:not-null]
                [createdPosix int #:not-null]
                [accessedPosix int #:not-null]
                #:constraints
                (primary-key corpusChecksumTableFasl cacheTokenizerRevisionFasl))
  (create-table tTokenizedCorporaDocuments
                #:columns
                [corpusChecksumTableFasl blob #:not-null]
                [cacheTokenizerRevisionFasl blob #:not-null]
                [docTitle text #:not-null]
                [docChecksum text #:not-null]
                #:constraints
                (primary-key corpusChecksumTableFasl cacheTokenizerRevisionFasl docTitle)
                (foreign-key corpusChecksumTableFasl cacheTokenizerRevisionFasl
                             #:references tTokenizedCorpora
                             #:on-delete #:cascade)
                (foreign-key docTitle docChecksum cacheTokenizerRevisionFasl
                             #:references tTokenizedDocuments
                             #:on-delete #:restrict)))

